[
  {
    "path": "posts/2020-12-27-equivalence-between-tikhonov-regularization-and-gaussian-priors/",
    "title": "The Special Equivalence between Tikhonov Regularization and Gaussian Priors",
    "description": "Straddling Bayesian and Frequentist Statistics.",
    "author": [
      {
        "name": "Jeffrey C. Wong",
        "url": {}
      }
    ],
    "date": "2020-12-27",
    "categories": [],
    "contents": "\nIn this post we will show that the maximum a-posteriori (MAP) estimator of a normal-normal is equal to the estimator from Tikhonov regularization.\nIntroduction\nThroughout this post we will build on ordinary least squares. First, we will assume that there is a random variable, \\(y\\), that is normally distributed and its mean is a linear combination of features, \\(x\\), so that \\(Y \\sim N(x^T\\beta, \\Sigma)\\).\nOptionally, the parameter vector \\(\\beta\\) can have a prior on it, in the form \\(\\beta \\sim N(\\mu_0, \\Sigma_0)\\).\nMaximum Likelihood for Normally Distributed Data\nIn frequentist statistics, we will write the likelihood of the data, then find an estimate of the parameters that will maximize the likelihood. The likelihood as a function of \\(\\beta\\) is\n\\[ L(\\beta) = \\prod_i N(y_i | x_i, \\beta, \\Sigma) = \\prod_i \\frac{1}{\\sqrt{(2 \\pi)^k |\\Sigma|}} \nexp({-\\frac{1}{2} (y_i - x_i^T \\beta)^T \\Sigma^{-1} (y_i - x_i^T \\beta)}).\\] The MLE estimate for \\(\\beta\\) will maximize the log-likelihood with respect to \\(\\beta\\), by differentiating it and finding its root. This produces the MLE estimate\n\\[\\hat{\\beta}^{MLE} = (X^T X)^{-1} X^T y.\\]\nMaximum a Posteriori\nWhen there is a gaussian prior in the form \\(\\beta \\sim N(\\mu_0, \\Sigma_0)\\), we use Baye’s rule to multiply the likelihood with the prior to get the posterior probability of \\(\\beta\\). Since we are multiplying two normals, we can add their exponents. The posterior takes the form of another normal distribution.\n\\[\\begin{align}\np(\\beta|y, x, \\Sigma) &= \\prod_i N(y_i | x_i, \\beta, \\Sigma) \\cdot N(\\beta | \\mu_0, \\Sigma_0) \\\\\n  &\\propto\n  \\prod_i \\frac{1}{|\\Sigma|} \n  exp({-\\frac{1}{2} \\big((y_i - x_i^T \\beta)^T \\Sigma^{-1} (y_i - x_i^T \\beta) - (\\beta - \\mu_0)^T \\Sigma_0^{-1} (\\beta - \\mu_0)\\big)}).\n\\end{align}\\]\nThe posterior turns out to be another normal distribution, \\(N(\\mu_1, \\Sigma_1)\\) (wiki), where\n\\[\\begin{align}\n\\Sigma_1 &= (\\Sigma_0^{-1} + n \\Sigma^{-1})^{-1} \\\\\n\\mu_1 &= \\Sigma_1 (\\Sigma_0^{-1} \\mu_0 + \\Sigma^{-1} \\sum_i{y_i})\n\\end{align}\\]\nThe maximum a-posteriori estimator (wiki) estimates the parameter vector as the mode of the posterior distribution. This is done by differentiating the posterior and solvings its root, similar to MLE. Taking the log posterior probability and then maximizing it gives\n\\[\\hat{\\beta}^{MAP} = \\arg max_{\\beta} \n- (y-X\\beta)^T \\Sigma^{-1} (y-X\\beta)\n- (\\beta-\\beta_0)^T \\Sigma_0^{-1} (\\beta-\\beta_0).\\] Recall that \\(\\Sigma\\) is fixed, and \\(\\mu_0\\) and \\(\\Sigma_0\\) are inputs for the prior. Differentiating and solving, we can show the MAP estimator is equal to Tikhonov regularization.\n\\[\\hat{\\beta}^{MAP} = (X^T X + \\Sigma_0)^{-1} (X^T y + \\Sigma_0 \\mu_0).\\]\nEquivalence between MLE and MAP\nWhen the prior is a constant everywhere, it factors out of the posterior probability as a constant. That means the MLE estimator is a special case of MAP when the prior is a uniform distribution.\n\n\n\n",
    "preview": {},
    "last_modified": "2020-12-29T10:56:49-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-12-25-choose-distill-for-writing-a-tech-blog/",
    "title": "Choose RMarkdown + Distill for writing a Tech Blog",
    "description": "RMarkdown and Distill are powerful tools for scientific communication.",
    "author": [
      {
        "name": "Jeffrey C. Wong",
        "url": {}
      }
    ],
    "date": "2020-12-25",
    "categories": [],
    "contents": "\nRMarkdown is a Champion for Scientific Communication\nRMarkdown has had a long history in scientific communication. Using RMarkdown, you can create beautifully rendered technical documents that can be hosted online or sent as pdfs. The documents weave together the output from code, latex, as well as other web tools like html, css, and javascript. By putting data analysis code right next to text, the author can describe and discuss individual steps in analysis, and show how to reproduce it using software. RMarkdown allows authors to construct an entire scientific narrative with the spirit of the scientific method. It facilitates this approach to scientific communication by creating great development flow between code and discussion: the code you write for development is the same code that you share to others to let them read and follow along.\nHere are examples of how RMarkdown embeds essential parts of scientific communication into one environment. First, we can write latex.\n\\[ \\int_0^1 x dx = \\frac{1}{2}.\\]\nNext, we can illustrate source code for functions\n\n\nfoo = function(x, y) {\n  (x-y)^2 %>% sum() %>% sqrt()\n}\n\n\n\nFinally, we can execute code inline, and can also generate visualizations.\n\n\na = 5\nb = a^2\nprint(b)\n\n\n[1] 25\n\n\n\n\nDistill is a Better Publishing Platform for Research\nDistill is both a publishing framework and machine learning research journal. It is advancing scientific communication by breaking the barriers of traditional pdf documents. Distill articles are html pages that allow publishing new kinds of scientific narratives, including those that have interactive visualizations, videos, and demos. It also follows scientific writing, listing the authors, date published, references, footnotes, and an appendix.\nDevelopers at RStudio have integrated Distill articles into RMarkdown with the #rstats Distill package. RMarkdown users can create Distill articles that can be submitted to the research journal. In addition, they’ve made it easy to collate Distill html articles into an online blog. This further enhances scientific communication because your development environment can also be used for online publishing and content management, creating a single end-to-end environment for development, sharing, and publishing. The Distill package creates the listing page that indexes all blog posts, adds a search bar to the blog, and adds comments and share links to each blog post. You can also link to the underlying RMarkdown source code to show how the blog post was generated. In this way, the blog post turns into a technical document that embodies RMarkdown’s reproducible research features.\nA Simple End-to-End Tech Stack\nA full Distill blog can be hosted using as few as two components, that are completely free. Publishing can be done without maintaining a server or a database.\nRMarkdown, and Distill to build the blog. When the blog is built, a series of .html files are generated that can be uploaded to a webhost.\ngithub pages can host the blog online for free.\n\n\n\n",
    "preview": "posts/2020-12-25-choose-distill-for-writing-a-tech-blog/choose-distill-for-writing-a-tech-blog_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2020-12-29T10:30:11-08:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  }
]
